{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SpamDetection_using_DL_KerasAPI.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mVCleiQn7o-o"
      },
      "source": [
        "In this workbook, I have created a deep learning model using Keras API. \n",
        "\n",
        "I have used an embedded layer which creates the word2vec vectors for the feature texts. Then added a LSTM layer followed by fully connected layer. The accuracy of the model on new data is 89% \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PE8q2zLARI4c"
      },
      "source": [
        "### Importing the required modules/packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LswEEonkRI4k",
        "outputId": "0060527f-1ff8-4792-9ee2-10758ca1cf0f"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "import string\n",
        "from sklearn.metrics import precision_recall_fscore_support as score\n",
        "from sklearn.pipeline import Pipeline, FeatureUnion, TransformerMixin\n",
        "#from mlxtend.feature_selection import ColumnSelector\n",
        "from sklearn.compose import ColumnTransformer\n",
        "nltk.download('stopwords')\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Embedding, LSTM, Dense, Dropout\n",
        "from keras.initializers import Constant\n",
        "from keras.optimizers import Adam"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cTQo0z49RI4l"
      },
      "source": [
        "### Loading file and looking into the dimensions of data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 240
        },
        "id": "BdbQiaN0RI4l",
        "outputId": "5f5de4a2-9bde-46f4-c93e-7af50ca316f3"
      },
      "source": [
        "raw_data = pd.read_csv(\"SMSSpamData.csv\",names=['label','text'], encoding='ISO-8859-1')\n",
        "pd.set_option('display.max_colwidth',100)\n",
        "\n",
        "\n",
        "print(f\"Shape of Data --> {raw_data.shape}\\n\")\n",
        "#print(pd.crosstab(raw_data['label'],columns = 'label'))\n",
        "#pd.crosstab(raw_data['label'],columns = 'label',normalize=True)\n",
        "raw_data.head()"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shape of Data --> (5572, 2)\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>label</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>ham</td>\n",
              "      <td>Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there g...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>ham</td>\n",
              "      <td>Ok lar... Joking wif u oni...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>spam</td>\n",
              "      <td>Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>ham</td>\n",
              "      <td>U dun say so early hor... U c already then say...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>ham</td>\n",
              "      <td>Nah I don't think he goes to usf, he lives around here though</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  label                                                                                                 text\n",
              "0   ham  Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there g...\n",
              "1   ham                                                                        Ok lar... Joking wif u oni...\n",
              "2  spam  Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive ...\n",
              "3   ham                                                    U dun say so early hor... U c already then say...\n",
              "4   ham                                        Nah I don't think he goes to usf, he lives around here though"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3XH5CltGnvaR"
      },
      "source": [
        "## **Data Processing**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eK58YKAHRI4n"
      },
      "source": [
        "### Functions to Create new features and cleaning the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7eEM4AN8RI4n"
      },
      "source": [
        "## Percentage Count\n",
        "def punct_pc(text):\n",
        "    punct_count = sum([1 for char in text if char in string.punctuation])\n",
        "    return (punct_count/(len(text) - text.count(' ')))*100\n",
        "\n",
        "## Stem\n",
        "def clean_data(text):\n",
        "    punct = \"\".join([word.lower() for word in text if word not in string.punctuation])\n",
        "    splt = re.split('\\W+',punct)\n",
        "    txt = [nltk.PorterStemmer().stem(word) for word in splt if word not in nltk.corpus.stopwords.words('english')]\n",
        "    return txt\n",
        "\n",
        "    \n"
      ],
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RHGl0T5pRI4n"
      },
      "source": [
        "### Train and Test Split "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IsJsO7V8RI4o"
      },
      "source": [
        "## Splitting the Data using Test size 0.2\n",
        "X_train,X_test,y_train,y_test = train_test_split(raw_data[['text']],raw_data[['label']],test_size=0.2,random_state=23)"
      ],
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "SYcpKisMdYxs",
        "outputId": "118cd4d5-4892-4c5d-9436-f8656bfa0d94"
      },
      "source": [
        "y_train"
      ],
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1627</th>\n",
              "      <td>spam</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4608</th>\n",
              "      <td>ham</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2709</th>\n",
              "      <td>ham</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>639</th>\n",
              "      <td>ham</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4544</th>\n",
              "      <td>ham</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2998</th>\n",
              "      <td>ham</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1993</th>\n",
              "      <td>ham</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1064</th>\n",
              "      <td>ham</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4838</th>\n",
              "      <td>ham</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>595</th>\n",
              "      <td>ham</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>4457 rows × 1 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "     label\n",
              "1627  spam\n",
              "4608   ham\n",
              "2709   ham\n",
              "639    ham\n",
              "4544   ham\n",
              "...    ...\n",
              "2998   ham\n",
              "1993   ham\n",
              "1064   ham\n",
              "4838   ham\n",
              "595    ham\n",
              "\n",
              "[4457 rows x 1 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eZJNfG-sHLjM"
      },
      "source": [
        "## Covert target to binary\n",
        "y_train = np.where(y_train == 'spam', 1,0)\n",
        "y_test = np.where(y_test == 'spam', 1,0)"
      ],
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g1w7ux3qmKmY"
      },
      "source": [
        "######################## Create new features Train and Test Data ########################\n",
        "#- Two new features are created - \n",
        "#- 1) text_length (the total length of the text)\n",
        "#- 2) Punct_pc (the percentage of punctuations in the text)\n",
        "\n",
        "## Train Data\n",
        "X_train[\"punct_pc\"] = X_train[\"text\"].apply(lambda x: punct_pc(x))\n",
        "X_train[\"text_length\"] = X_train[\"text\"].apply(lambda x: len(x)-x.count(' '))\n",
        "\n",
        "## Test Data\n",
        "X_test[\"punct_pc\"] = X_test[\"text\"].apply(lambda x: punct_pc(x))\n",
        "X_test[\"text_length\"] = X_test[\"text\"].apply(lambda x: len(x)-x.count(' '))"
      ],
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qkCDrmmeRI4o"
      },
      "source": [
        "### Pipeline to process the Text Data\n",
        "\n",
        "- Tokenization\n",
        "- Cleaning\n",
        "- Normalization\n",
        "- Lemmatization\n",
        "- Steaming"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J5ytibkoZRXc"
      },
      "source": [
        "Custom Transformer to select the Columns"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3I3H3jnWYhO5"
      },
      "source": [
        "## Customer transformer to Select features \n",
        "class ColumnExtractor(TransformerMixin):\n",
        "    def __init__(self, cols):\n",
        "        self.cols = cols\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        # stateless transformer\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        # assumes X is a DataFrame\n",
        "        Xcols = pd.Series(X[self.cols])\n",
        "        return (Xcols)\n"
      ],
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N0a2j7x32y7l"
      },
      "source": [
        "import numpy as np\n",
        "import multiprocessing as mp\n",
        "\n",
        "import string\n",
        "import spacy \n",
        "import en_core_web_sm\n",
        "from nltk.tokenize import word_tokenize\n",
        "from sklearn.base import TransformerMixin, BaseEstimator\n",
        "\n",
        "\n",
        "nlp = en_core_web_sm.load()\n",
        "\n",
        "\n",
        "class TextPreprocessor(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self,\n",
        "                 \n",
        "                    variety=\"BrE\",\n",
        "                 user_abbrevs={},\n",
        "                 n_jobs=1):\n",
        "        \"\"\"\n",
        "        Text preprocessing transformer includes steps:\n",
        "            1. Text normalization\n",
        "            2. Punctuation removal\n",
        "            3. Stop words removal\n",
        "            4. Lemmatization\n",
        "        \n",
        "        variety - format of date (AmE - american type, BrE - british format) \n",
        "        user_abbrevs - dict of user abbreviations mappings (from normalise package)\n",
        "        n_jobs - parallel jobs to run\n",
        "        \"\"\"\n",
        "        self.variety = variety\n",
        "        self.user_abbrevs = user_abbrevs\n",
        "        self.n_jobs = n_jobs\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "\n",
        "    def transform(self, X, *_):\n",
        "        X_copy = X.copy()\n",
        "\n",
        "        partitions = 1\n",
        "        cores = mp.cpu_count()\n",
        "        if self.n_jobs <= -1:\n",
        "            partitions = cores\n",
        "        elif self.n_jobs <= 0:\n",
        "            return X_copy.apply(self._preprocess_text)\n",
        "        else:\n",
        "            partitions = min(self.n_jobs, cores)\n",
        "\n",
        "        data_split = np.array_split(X_copy, partitions)\n",
        "        pool = mp.Pool(cores)\n",
        "        data = pd.concat(pool.map(self._preprocess_part, data_split))\n",
        "        pool.close()\n",
        "        pool.join()\n",
        "\n",
        "        return data\n",
        "\n",
        "    def _preprocess_part(self, part):\n",
        "        return part.apply(self._preprocess_text)\n",
        "\n",
        "    def _preprocess_text(self, text):\n",
        "        normalized_text = self._normalize(text)\n",
        "        doc = nlp(normalized_text)\n",
        "\n",
        "        removed_punct = self._remove_punct(doc)\n",
        "        removed_stop_words = self._remove_stop_words(removed_punct)\n",
        "        return self._lemmatize(removed_stop_words)\n",
        "\n",
        "    def _normalize(self, text):\n",
        "        # some issues in normalise package\n",
        "        try:\n",
        "            return ' '.join(normalise(text, variety=self.variety, user_abbrevs=self.user_abbrevs, verbose=False))\n",
        "        except:\n",
        "            return text\n",
        "\n",
        "    def _remove_punct(self, doc):\n",
        "        return [t for t in doc if t.text not in string.punctuation]\n",
        "\n",
        "    def _remove_stop_words(self, doc):\n",
        "        return [t for t in doc if not t.is_stop]\n",
        "\n",
        "    def _lemmatize(self, doc):\n",
        "        return ' '.join([t.lemma_ for t in doc])\n",
        "\n",
        "\n",
        "\n",
        "num_cols = [\"punct_pc\",\"text_length\"]\n",
        "Column_trans = ColumnTransformer(\n",
        "     [('scaler', StandardScaler(),num_cols)],\n",
        "     remainder='drop')\n",
        "\n",
        "num_pipe = Pipeline([\n",
        "  ('scaler', Column_trans)\n",
        "  ])"
      ],
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 223
        },
        "id": "P8qu-WbsC7rv",
        "outputId": "1bcf5a02-f2ac-47d2-8c2d-35aa3fb505b1"
      },
      "source": [
        "X_train.head()"
      ],
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>punct_pc</th>\n",
              "      <th>text_length</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1627</th>\n",
              "      <td>You have been selected to stay in 1 of 250 top British hotels - FOR NOTHING! Holiday Worth å£350...</td>\n",
              "      <td>4.958678</td>\n",
              "      <td>121</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4608</th>\n",
              "      <td>Y de asking like this.</td>\n",
              "      <td>5.555556</td>\n",
              "      <td>18</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2709</th>\n",
              "      <td>Sorry de i went to shop.</td>\n",
              "      <td>5.263158</td>\n",
              "      <td>19</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>639</th>\n",
              "      <td>I had askd u a question some hours before. Its answer</td>\n",
              "      <td>2.325581</td>\n",
              "      <td>43</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4544</th>\n",
              "      <td>Never y lei... I v lazy... Got wat? Dat day Ì_ send me da url cant work one...</td>\n",
              "      <td>18.032787</td>\n",
              "      <td>61</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                                                                     text  ...  text_length\n",
              "1627  You have been selected to stay in 1 of 250 top British hotels - FOR NOTHING! Holiday Worth å£350...  ...          121\n",
              "4608                                                                               Y de asking like this.  ...           18\n",
              "2709                                                                             Sorry de i went to shop.  ...           19\n",
              "639                                                 I had askd u a question some hours before. Its answer  ...           43\n",
              "4544                       Never y lei... I v lazy... Got wat? Dat day Ì_ send me da url cant work one...  ...           61\n",
              "\n",
              "[5 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 80
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FQz7c8VIq3pZ"
      },
      "source": [
        "Preprocess_text = Pipeline([(\"select_text\", ColumnExtractor(cols=\"text\")),\n",
        "                            ('preprocess', TextPreprocessor())\n",
        "                            ])\n",
        "\n",
        "preprocessing_pipeline = Pipeline([\n",
        "    ('feat_union', FeatureUnion(transformer_list=[\n",
        "          ('text_pipeline', Preprocess_text),\n",
        "          ('num_pipeline', num_pipe)\n",
        "          ]))\n",
        "    ])"
      ],
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ShBE5VnSJ41M"
      },
      "source": [
        "**Preprocess the Train and Test data using the pipeline**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nXxWEIR6sZCW"
      },
      "source": [
        "X_train_preprocessed = Preprocess_text.fit_transform(X_train)\n",
        "X_test_preprocessed = Preprocess_text.fit_transform(X_test)"
      ],
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qIrxgWFdKLh5"
      },
      "source": [
        "Calculate the size of the vocabulary (i.e the number of unique words in the entire corpus) . This is needed as a paramter \"input_dim\" in the embedded layer "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Prq5ONM8AbOt"
      },
      "source": [
        "from collections import Counter\n",
        "# Count unique words\n",
        "def counter_word(text):\n",
        "    count = Counter()\n",
        "    for i in text.values:\n",
        "        for word in i.split():\n",
        "            count[word] += 1\n",
        "    return count\n",
        "\n",
        "counter = counter_word(X_train_preprocessed)\n",
        "vocab_size = len(counter)"
      ],
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FocS8402AjbL"
      },
      "source": [
        "# We are taking a maximum lenght for the sequence as 30 (sequence above this length will be trimmed down and \n",
        "# and below this lenght will be padded with zeroes)\n",
        "max_length = 30\n",
        "num_words = vocab_size + 1000"
      ],
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FCqUJhX1NUWd"
      },
      "source": [
        "Using Tokenizer class from keras tokenize the sentences. Tokinzer assignes a unique ID for each word in the entire trainign set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VCXzdA9eFWSo"
      },
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "tokenizer = Tokenizer(num_words=num_words)\n",
        "tokenizer.fit_on_texts(X_train_preprocessed)\n",
        "X_train_sequences = tokenizer.texts_to_sequences(X_train_preprocessed)"
      ],
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jpVvm8HQNnY_"
      },
      "source": [
        "## We can check the index of all the words created by the tokenizer\n",
        "word_index = tokenizer.word_index\n",
        "#print(word_index)"
      ],
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H8alft9sWC8N"
      },
      "source": [
        "Using pad_squeence, pad the sequences to have the same length"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TppbQLiDDe10"
      },
      "source": [
        "from keras.preprocessing.sequence import pad_sequences\n",
        "X_train_padded = pad_sequences(\n",
        "    X_train_sequences, maxlen=max_length, padding=\"post\", truncating=\"post\"\n",
        ")"
      ],
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ofCd1QWyFUP9"
      },
      "source": [
        "## Applying same tranformation to Test set\n",
        "test_sequences = tokenizer.texts_to_sequences(X_test_preprocessed)\n",
        "test_padded = pad_sequences(\n",
        "    test_sequences, maxlen=max_length, padding=\"post\", truncating=\"post\"\n",
        ")"
      ],
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bQdLhLFAWhYE"
      },
      "source": [
        "## Deep Learning model training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vas585qE6xaQ"
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Embedding, LSTM, Dense, Dropout\n",
        "from keras.initializers import Constant\n",
        "from keras.optimizers import Adam\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "model.add(Embedding(num_words, 32, input_length=max_length))\n",
        "model.add(LSTM(64, dropout=0.1))\n",
        "model.add(Dense(1, activation=\"sigmoid\"))\n",
        "\n",
        "\n",
        "optimizer = Adam(learning_rate=3e-4)\n",
        "\n",
        "model.compile(loss=\"binary_crossentropy\", optimizer=optimizer, metrics=[\"accuracy\"])"
      ],
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l5m_JUtCDEYN",
        "outputId": "67632d78-0226-4c08-bfc4-b03c19182b57"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, 30, 32)            297408    \n",
            "_________________________________________________________________\n",
            "lstm (LSTM)                  (None, 64)                24832     \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 1)                 65        \n",
            "=================================================================\n",
            "Total params: 322,305\n",
            "Trainable params: 322,305\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "67t-nna6DJzM",
        "outputId": "ff7600c8-570f-4e1c-85f8-4b09faa89489"
      },
      "source": [
        "Trained_model = model.fit(\n",
        "    X_train_padded, y_train, epochs=10, validation_data=(test_padded, y_test)\n",
        ")"
      ],
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "140/140 [==============================] - 6s 29ms/step - loss: 0.5252 - accuracy: 0.7927 - val_loss: 0.1318 - val_accuracy: 0.9561\n",
            "Epoch 2/10\n",
            "140/140 [==============================] - 3s 25ms/step - loss: 0.0875 - accuracy: 0.9763 - val_loss: 0.0526 - val_accuracy: 0.9883\n",
            "Epoch 3/10\n",
            "140/140 [==============================] - 3s 25ms/step - loss: 0.0274 - accuracy: 0.9961 - val_loss: 0.0459 - val_accuracy: 0.9874\n",
            "Epoch 4/10\n",
            "140/140 [==============================] - 4s 26ms/step - loss: 0.0160 - accuracy: 0.9972 - val_loss: 0.0501 - val_accuracy: 0.9883\n",
            "Epoch 5/10\n",
            "140/140 [==============================] - 4s 26ms/step - loss: 0.0083 - accuracy: 0.9984 - val_loss: 0.0436 - val_accuracy: 0.9883\n",
            "Epoch 6/10\n",
            "140/140 [==============================] - 4s 26ms/step - loss: 0.0081 - accuracy: 0.9985 - val_loss: 0.0520 - val_accuracy: 0.9883\n",
            "Epoch 7/10\n",
            "140/140 [==============================] - 4s 26ms/step - loss: 0.0029 - accuracy: 0.9996 - val_loss: 0.0507 - val_accuracy: 0.9883\n",
            "Epoch 8/10\n",
            "140/140 [==============================] - 4s 26ms/step - loss: 0.0022 - accuracy: 0.9999 - val_loss: 0.0604 - val_accuracy: 0.9874\n",
            "Epoch 9/10\n",
            "140/140 [==============================] - 4s 26ms/step - loss: 0.0018 - accuracy: 0.9999 - val_loss: 0.0639 - val_accuracy: 0.9874\n",
            "Epoch 10/10\n",
            "140/140 [==============================] - 4s 26ms/step - loss: 0.0017 - accuracy: 0.9999 - val_loss: 0.0679 - val_accuracy: 0.9865\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ETE5eayxVeHF"
      },
      "source": [
        "model.save(\"trained_model.h5\")"
      ],
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7DWQuCGBX6Dn"
      },
      "source": [
        "# **Predict on New Test Data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d_1fLL-eV7k7"
      },
      "source": [
        "test_data = pd.read_csv(\"Test_Emails.csv\",names=['label','text'], encoding='ISO-8859-1')"
      ],
      "execution_count": 106,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "VybxUHg6YLPg",
        "outputId": "6dd3b2c5-ff2a-47eb-9854-3453c1b6d02f"
      },
      "source": [
        "test_data"
      ],
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>label</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>ham</td>\n",
              "      <td>Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there g...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>ham</td>\n",
              "      <td>Ok lar... Joking wif u oni...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>spam</td>\n",
              "      <td>Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>ham</td>\n",
              "      <td>U dun say so early hor... U c already then say...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>ham</td>\n",
              "      <td>Nah I don't think he goes to usf, he lives around here though</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>490</th>\n",
              "      <td>ham</td>\n",
              "      <td>Sorry man my account's dry or I would, if you want we could trade back half or I could buy some ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>491</th>\n",
              "      <td>spam</td>\n",
              "      <td>Congrats! 1 year special cinema pass for 2 is yours. call 09061209465 now! C Suprman V, Matrix3,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>492</th>\n",
              "      <td>ham</td>\n",
              "      <td>Sorry,in meeting I'll call later</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>493</th>\n",
              "      <td>ham</td>\n",
              "      <td>What class of &amp;lt;#&amp;gt; reunion?</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>494</th>\n",
              "      <td>ham</td>\n",
              "      <td>Are you free now?can i call now?</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>495 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "    label                                                                                                 text\n",
              "0     ham  Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there g...\n",
              "1     ham                                                                        Ok lar... Joking wif u oni...\n",
              "2    spam  Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive ...\n",
              "3     ham                                                    U dun say so early hor... U c already then say...\n",
              "4     ham                                        Nah I don't think he goes to usf, he lives around here though\n",
              "..    ...                                                                                                  ...\n",
              "490   ham  Sorry man my account's dry or I would, if you want we could trade back half or I could buy some ...\n",
              "491  spam  Congrats! 1 year special cinema pass for 2 is yours. call 09061209465 now! C Suprman V, Matrix3,...\n",
              "492   ham                                                                     Sorry,in meeting I'll call later\n",
              "493   ham                                                                     What class of &lt;#&gt; reunion?\n",
              "494   ham                                                                     Are you free now?can i call now?\n",
              "\n",
              "[495 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 110
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bJR255_kY9YP"
      },
      "source": [
        "new_X_test = test_data.loc[:,[\"text\"]]\n",
        "new_Y_test = test_data.loc[:,[\"label\"]]"
      ],
      "execution_count": 115,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "aIjmIiXwL99A",
        "outputId": "f50c6811-444a-416e-8e3b-58cbfbbad44a"
      },
      "source": [
        "new_Y_test"
      ],
      "execution_count": 116,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>ham</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>ham</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>spam</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>ham</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>ham</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>490</th>\n",
              "      <td>ham</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>491</th>\n",
              "      <td>spam</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>492</th>\n",
              "      <td>ham</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>493</th>\n",
              "      <td>ham</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>494</th>\n",
              "      <td>ham</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>495 rows × 1 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "    label\n",
              "0     ham\n",
              "1     ham\n",
              "2    spam\n",
              "3     ham\n",
              "4     ham\n",
              "..    ...\n",
              "490   ham\n",
              "491  spam\n",
              "492   ham\n",
              "493   ham\n",
              "494   ham\n",
              "\n",
              "[495 rows x 1 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 116
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uv3E3qLRgOtI",
        "outputId": "fe9cbcb5-7d7c-4c67-9020-9a177e38f679"
      },
      "source": [
        "new_X_test.shape"
      ],
      "execution_count": 117,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(495, 1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 117
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BsTeiSLGcYLW"
      },
      "source": [
        "Y_test = np.where(new_Y_test.label == 'spam', 1, 0)"
      ],
      "execution_count": 120,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DO5IhYkc7FSk"
      },
      "source": [
        "**Preprocess Test data by fitting in the preprocess pipeline**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j45E5kqLZI4l"
      },
      "source": [
        "## Fit X_test in pipeline\n",
        "new_X_test_processes = Preprocess_text.fit_transform(new_X_test)\n",
        "\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "tokenizer = Tokenizer(num_words=num_words)\n",
        "tokenizer.fit_on_texts(new_X_test_processes)\n",
        "new_X_test_sequences = tokenizer.texts_to_sequences(new_X_test_processes)\n",
        "\n",
        "\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "new_X_test_padded = pad_sequences(\n",
        "    new_X_test_sequences, maxlen=max_length, padding=\"post\", truncating=\"post\"\n",
        ")"
      ],
      "execution_count": 102,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OFiEiLSKZy4V",
        "outputId": "bda45672-fac3-4140-e87f-03df593976c6"
      },
      "source": [
        "predict = model.predict_classes(new_X_test_padded)"
      ],
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
            "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hDnIfVJhcBJq",
        "outputId": "b8795e2a-0783-499e-d0e6-cd74a8d0eea8"
      },
      "source": [
        "## Performance metrics\n",
        "from sklearn.metrics import classification_report\n",
        "print(classification_report(Y_test,predict))"
      ],
      "execution_count": 122,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.86      0.90      0.88       424\n",
            "           1       0.15      0.11      0.13        71\n",
            "\n",
            "    accuracy                           0.78       495\n",
            "   macro avg       0.51      0.50      0.50       495\n",
            "weighted avg       0.76      0.78      0.77       495\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dtpDYTvAcU9w",
        "outputId": "f549ab87-c7a2-48fb-d8dd-9b632fa7cdb3"
      },
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "print(confusion_matrix(Y_test,predict))"
      ],
      "execution_count": 123,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[380  44]\n",
            " [ 63   8]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QKb2raMEMzbB"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}