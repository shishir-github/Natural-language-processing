{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mVCleiQn7o-o"
   },
   "source": [
    "In this workbook, I have created a deep learning model using Keras API. \n",
    "\n",
    "I have used an embedded layer which creates the word2vec vectors for the feature texts. Then added a LSTM layer followed by fully connected layer. The accuracy of the model on new data is 89% \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PE8q2zLARI4c"
   },
   "source": [
    "### Importing the required modules/packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LswEEonkRI4k",
    "outputId": "dd409b0f-5de7-4d15-d494-40d8e8cfc7f1"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\gunne\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "Keras requires TensorFlow 2.2 or higher. Install TensorFlow via `pip install tensorflow`",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mRandomRotation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-59bcf97789ce>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompose\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mColumnTransformer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'stopwords'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSequential\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mEmbedding\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mLSTM\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mDense\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mDropout\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minitializers\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mConstant\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     raise ImportError(\n\u001b[1;32m----> 6\u001b[1;33m         \u001b[1;34m'Keras requires TensorFlow 2.2 or higher. '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m         'Install TensorFlow via `pip install tensorflow`')\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: Keras requires TensorFlow 2.2 or higher. Install TensorFlow via `pip install tensorflow`"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "from sklearn.metrics import precision_recall_fscore_support as score\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion, TransformerMixin\n",
    "#from mlxtend.feature_selection import ColumnSelector\n",
    "from sklearn.compose import ColumnTransformer\n",
    "nltk.download('stopwords')\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "from keras.initializers import Constant\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cTQo0z49RI4l"
   },
   "source": [
    "### Loading file and looking into the dimensions of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 240
    },
    "id": "BdbQiaN0RI4l",
    "outputId": "e134b83c-68cb-4e0d-e1e0-5161d65b16e3"
   },
   "outputs": [],
   "source": [
    "raw_data = pd.read_csv(\"SMSSpamData.csv\",names=['label','text'])\n",
    "pd.set_option('display.max_colwidth',100)\n",
    "\n",
    "\n",
    "print(f\"Shape of Data --> {raw_data.shape}\\n\")\n",
    "#print(pd.crosstab(raw_data['label'],columns = 'label'))\n",
    "#pd.crosstab(raw_data['label'],columns = 'label',normalize=True)\n",
    "raw_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 143
    },
    "id": "NtPGU3VgRI4m",
    "outputId": "c74fd853-0214-4f1e-ea26-3fd6bdd61cdc"
   },
   "outputs": [],
   "source": [
    "### Label distrbution (Ham is messaged which are not Spam)\n",
    "pd.crosstab(raw_data['label'],columns = 'label',normalize=True)\n",
    "#raw_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3XH5CltGnvaR"
   },
   "source": [
    "## **Data Processing**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eK58YKAHRI4n"
   },
   "source": [
    "### Functions to Create new features and cleaning the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7eEM4AN8RI4n"
   },
   "outputs": [],
   "source": [
    "## Percentage Count\n",
    "def punct_pc(text):\n",
    "    punct_count = sum([1 for char in text if char in string.punctuation])\n",
    "    return (punct_count/(len(text) - text.count(' ')))*100\n",
    "\n",
    "## Stem\n",
    "def clean_data(text):\n",
    "    punct = \"\".join([word.lower() for word in text if word not in string.punctuation])\n",
    "    splt = re.split('\\W+',punct)\n",
    "    txt = [nltk.PorterStemmer().stem(word) for word in splt if word not in nltk.corpus.stopwords.words('english')]\n",
    "    return txt\n",
    "\n",
    "## CAPS \n",
    "def count_caps(text):\n",
    "    \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RHGl0T5pRI4n"
   },
   "source": [
    "### Train and Test Split "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IsJsO7V8RI4o"
   },
   "outputs": [],
   "source": [
    "## Splitting the Data using Test size 0.2\n",
    "X_train,X_test,y_train,y_test = train_test_split(raw_data[['text',]],raw_data['label'],test_size=0.2,random_state=23)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SYcpKisMdYxs",
    "outputId": "a58b303d-4f11-4100-e602-addefa4512e3"
   },
   "outputs": [],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eZJNfG-sHLjM"
   },
   "outputs": [],
   "source": [
    "## Covert target to binary\n",
    "y_train = np.where(y_train == 'spam', 1,0)\n",
    "y_test = np.where(y_test == 'spam', 1,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g1w7ux3qmKmY"
   },
   "outputs": [],
   "source": [
    "######################## Create new features Train and Test Data ########################\n",
    "#- Two new features are created - \n",
    "#- 1) text_length (the total length of the text)\n",
    "#- 2) Punct_pc (the percentage of punctuations in the text)\n",
    "\n",
    "## Train Data\n",
    "X_train[\"punct_pc\"] = X_train[\"text\"].apply(lambda x: punct_pc(x))\n",
    "X_train[\"text_length\"] = X_train[\"text\"].apply(lambda x: len(x)-x.count(' '))\n",
    "\n",
    "## Test Data\n",
    "X_test[\"punct_pc\"] = X_test[\"text\"].apply(lambda x: punct_pc(x))\n",
    "X_test[\"text_length\"] = X_test[\"text\"].apply(lambda x: len(x)-x.count(' '))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qkCDrmmeRI4o"
   },
   "source": [
    "### Pipeline to process the Text Data\n",
    "\n",
    "- Tokenization\n",
    "- Cleaning\n",
    "- Normalization\n",
    "- Lemmatization\n",
    "- Steaming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J5ytibkoZRXc"
   },
   "source": [
    "Custom Transformer to select the Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3I3H3jnWYhO5"
   },
   "outputs": [],
   "source": [
    "## Customer transformer to Select features \n",
    "class ColumnExtractor(TransformerMixin):\n",
    "    def __init__(self, cols):\n",
    "        self.cols = cols\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        # stateless transformer\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        # assumes X is a DataFrame\n",
    "        Xcols = pd.Series(X[self.cols])\n",
    "        return (Xcols)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N0a2j7x32y7l"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import multiprocessing as mp\n",
    "\n",
    "import string\n",
    "import spacy \n",
    "import en_core_web_sm\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.base import TransformerMixin, BaseEstimator\n",
    "\n",
    "\n",
    "nlp = en_core_web_sm.load()\n",
    "\n",
    "\n",
    "class TextPreprocessor(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self,\n",
    "                 \n",
    "                    variety=\"BrE\",\n",
    "                 user_abbrevs={},\n",
    "                 n_jobs=1):\n",
    "        \"\"\"\n",
    "        Text preprocessing transformer includes steps:\n",
    "            1. Text normalization\n",
    "            2. Punctuation removal\n",
    "            3. Stop words removal\n",
    "            4. Lemmatization\n",
    "        \n",
    "        variety - format of date (AmE - american type, BrE - british format) \n",
    "        user_abbrevs - dict of user abbreviations mappings (from normalise package)\n",
    "        n_jobs - parallel jobs to run\n",
    "        \"\"\"\n",
    "        self.variety = variety\n",
    "        self.user_abbrevs = user_abbrevs\n",
    "        self.n_jobs = n_jobs\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, *_):\n",
    "        X_copy = X.copy()\n",
    "\n",
    "        partitions = 1\n",
    "        cores = mp.cpu_count()\n",
    "        if self.n_jobs <= -1:\n",
    "            partitions = cores\n",
    "        elif self.n_jobs <= 0:\n",
    "            return X_copy.apply(self._preprocess_text)\n",
    "        else:\n",
    "            partitions = min(self.n_jobs, cores)\n",
    "\n",
    "        data_split = np.array_split(X_copy, partitions)\n",
    "        pool = mp.Pool(cores)\n",
    "        data = pd.concat(pool.map(self._preprocess_part, data_split))\n",
    "        pool.close()\n",
    "        pool.join()\n",
    "\n",
    "        return data\n",
    "\n",
    "    def _preprocess_part(self, part):\n",
    "        return part.apply(self._preprocess_text)\n",
    "\n",
    "    def _preprocess_text(self, text):\n",
    "        normalized_text = self._normalize(text)\n",
    "        doc = nlp(normalized_text)\n",
    "\n",
    "        removed_punct = self._remove_punct(doc)\n",
    "        removed_stop_words = self._remove_stop_words(removed_punct)\n",
    "        return self._lemmatize(removed_stop_words)\n",
    "\n",
    "    def _normalize(self, text):\n",
    "        # some issues in normalise package\n",
    "        try:\n",
    "            return ' '.join(normalise(text, variety=self.variety, user_abbrevs=self.user_abbrevs, verbose=False))\n",
    "        except:\n",
    "            return text\n",
    "\n",
    "    def _remove_punct(self, doc):\n",
    "        return [t for t in doc if t.text not in string.punctuation]\n",
    "\n",
    "    def _remove_stop_words(self, doc):\n",
    "        return [t for t in doc if not t.is_stop]\n",
    "\n",
    "    def _lemmatize(self, doc):\n",
    "        return ' '.join([t.lemma_ for t in doc])\n",
    "\n",
    "\n",
    "\n",
    "num_cols = [\"punct_pc\",\"text_length\"]\n",
    "Column_trans = ColumnTransformer(\n",
    "     [('scaler', StandardScaler(),num_cols)],\n",
    "     remainder='drop')\n",
    "\n",
    "num_pipe = Pipeline([\n",
    "  ('scaler', Column_trans)\n",
    "  ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 223
    },
    "id": "P8qu-WbsC7rv",
    "outputId": "a2a3976e-0265-4a5c-b4f3-8b8e21f05730"
   },
   "outputs": [],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FQz7c8VIq3pZ"
   },
   "outputs": [],
   "source": [
    "Preprocess_text = Pipeline([(\"select_text\", ColumnExtractor(cols=\"text\")),\n",
    "                            ('preprocess', TextPreprocessor())\n",
    "                            ])\n",
    "\n",
    "preprocessing_pipeline = Pipeline([\n",
    "    ('feat_union', FeatureUnion(transformer_list=[\n",
    "          ('text_pipeline', Preprocess_text),\n",
    "          ('num_pipeline', num_pipe)\n",
    "          ]))\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ShBE5VnSJ41M"
   },
   "source": [
    "**Preprocess the Train and Test data using the pipeline**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nXxWEIR6sZCW"
   },
   "outputs": [],
   "source": [
    "X_train_preprocessed = Preprocess_text.fit_transform(X_train)\n",
    "X_test_preprocessed = Preprocess_text.fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qIrxgWFdKLh5"
   },
   "source": [
    "Calculate the size of the vocabulary (i.e the number of unique words in the entire corpus) . This is needed as a paramter \"input_dim\" in the embedded layer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Prq5ONM8AbOt"
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "# Count unique words\n",
    "def counter_word(text):\n",
    "    count = Counter()\n",
    "    for i in text.values:\n",
    "        for word in i.split():\n",
    "            count[word] += 1\n",
    "    return count\n",
    "\n",
    "counter = counter_word(X_train_preprocessed)\n",
    "vocab_size = len(counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FocS8402AjbL"
   },
   "outputs": [],
   "source": [
    "# We are taking a maximum lenght for the sequence as 30 (sequence above this length will be trimmed down and \n",
    "# and below this lenght will be padded with zeroes)\n",
    "max_length = 30\n",
    "num_words = vocab_size + 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FCqUJhX1NUWd"
   },
   "source": [
    "Using Tokenizer class from keras tokenize the sentences. Tokinzer assignes a unique ID for each word in the entire trainign set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VCXzdA9eFWSo"
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "tokenizer = Tokenizer(num_words=num_words)\n",
    "tokenizer.fit_on_texts(X_train_preprocessed)\n",
    "X_train_sequences = tokenizer.texts_to_sequences(X_train_preprocessed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jpVvm8HQNnY_",
    "outputId": "73bc7a43-1d0a-4f94-dd10-56d1bab77ea4"
   },
   "outputs": [],
   "source": [
    "## We can check the index of all the words created by the tokenizer\n",
    "word_index = tokenizer.word_index\n",
    "#print(word_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H8alft9sWC8N"
   },
   "source": [
    "Using pad_squeence, pad the sequences to have the same length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TppbQLiDDe10"
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "X_train_padded = pad_sequences(\n",
    "    X_train_sequences, maxlen=max_length, padding=\"post\", truncating=\"post\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ofCd1QWyFUP9"
   },
   "outputs": [],
   "source": [
    "## Applying same tranformation to Test set\n",
    "test_sequences = tokenizer.texts_to_sequences(X_test_preprocessed)\n",
    "test_padded = pad_sequences(\n",
    "    test_sequences, maxlen=max_length, padding=\"post\", truncating=\"post\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bQdLhLFAWhYE"
   },
   "source": [
    "## Deep Learning model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Vas585qE6xaQ"
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "from keras.initializers import Constant\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Embedding(num_words, 32, input_length=max_length))\n",
    "model.add(LSTM(64, dropout=0.1))\n",
    "model.add(Dense(1, activation=\"sigmoid\"))\n",
    "\n",
    "\n",
    "optimizer = Adam(learning_rate=3e-4)\n",
    "\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=optimizer, metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "l5m_JUtCDEYN",
    "outputId": "e7e6debc-64a4-4253-c80a-55800df24ee3"
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "67t-nna6DJzM",
    "outputId": "421ebf3f-5bfc-418a-f218-51d7916828dc"
   },
   "outputs": [],
   "source": [
    "Trained_model = model.fit(\n",
    "    X_train_padded, y_train, epochs=10, validation_data=(test_padded, y_test)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ETE5eayxVeHF"
   },
   "outputs": [],
   "source": [
    "model.save(\"trained_model.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7DWQuCGBX6Dn"
   },
   "source": [
    "# **Predict on New Test Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d_1fLL-eV7k7"
   },
   "outputs": [],
   "source": [
    "test_data = pd.read_csv(\"Test_Emails.csv\",names=['label','text'], encoding='ISO-8859-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VybxUHg6YLPg",
    "outputId": "c8af3a7c-67ba-4878-bb81-fe09ea1f30da"
   },
   "outputs": [],
   "source": [
    "test_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bJR255_kY9YP"
   },
   "outputs": [],
   "source": [
    "new_X_test = test_data.loc[:,[\"text\"]]\n",
    "new_Y_test = test_data.iloc[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Uv3E3qLRgOtI",
    "outputId": "a51d473a-5fb9-4869-c419-9e35b776a26d"
   },
   "outputs": [],
   "source": [
    "new_X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BsTeiSLGcYLW"
   },
   "outputs": [],
   "source": [
    "new_Y_test = np.where(Y_test == 'spam', 1, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DO5IhYkc7FSk"
   },
   "source": [
    "**Preprocess Test data by fitting in the preprocess pipeline**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j45E5kqLZI4l"
   },
   "outputs": [],
   "source": [
    "## Fit X_test in pipeline\n",
    "new_X_test_processes = Preprocess_text.fit_transform(new_X_test)\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "tokenizer = Tokenizer(num_words=num_words)\n",
    "tokenizer.fit_on_texts(new_X_test_processes)\n",
    "new_X_test_sequences = tokenizer.texts_to_sequences(new_X_test_processes)\n",
    "\n",
    "\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "new_X_test_padded = pad_sequences(\n",
    "    new_X_test_sequences, maxlen=max_length, padding=\"post\", truncating=\"post\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OFiEiLSKZy4V",
    "outputId": "7491cd63-878b-4699-c92d-165d484a00d6"
   },
   "outputs": [],
   "source": [
    "predict = model.predict_classes(new_X_test_padded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hDnIfVJhcBJq",
    "outputId": "2b8df16a-21c5-4b36-a876-01970b35d816"
   },
   "outputs": [],
   "source": [
    "## Performance metrics\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(new_Y_test,predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dtpDYTvAcU9w",
    "outputId": "8005c4de-2055-4499-dc6e-c2c05ca27dfa"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "print(confusion_matrix(new_Y_test,predict))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Spam dectection using NLP and Keras.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
